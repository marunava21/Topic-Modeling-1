{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fb8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /ifxhome/manna/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['create job fa error recipe command scanning invalid general package at equipment for no container', 'out button grey transaction in is move track and buttons lot to spec job list', 'the lot to user in is but not step for still as be error and', 'bin data no code defined loss reason failure response error date for disable web lot', 'not belong does equipment this to the lot for is error add in validate material', 'affected number area server application integration contact site printer lot sin to move start waiting', 'quantity strip total map match error not lot does allow with out reject off is', 'test summary missing mode quantity flag bom in tab withdrawal column marking withdraw run through', 'print tag to hold lot of carrier than its more process set cannot be printed', 'submit log code client source get attached error connection invoice element respond root connected missing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "df_reviews=pd.read_excel('tickets (4).xlsx')\n",
    "# print(df_reviews)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "df_reviews.dropna(subset=['Desc'], how='all', inplace=True)\n",
    "\n",
    "# df_reviews=df_reviews.drop('id',axis=1)\n",
    "\n",
    "\n",
    "#remove all languages except english\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def keep_english(x):\n",
    "    englishwords=\" \".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in words or not w.isalpha())\n",
    "    return englishwords\n",
    "\n",
    "df_reviews['Desc']=df_reviews['Desc'].apply(keep_english)\n",
    "df_reviews['Desc']\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "# df_listing=pd.read_csv('/Users/sahilsharma/Desktop/BT5126/Group Project/Data/listing_ids.csv')\n",
    "# df_topic=pd.merge(df_reviews,df_listing,left_on='listing_id', right_on='id',how='inner')\n",
    "# df_topic=df_topic[['listing_id','comments']]\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "# df_topic=df_topic.sample(frac=0.4)\n",
    "# df_topic\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "vectorizer=TfidfVectorizer(max_df=0.85,min_df=10)\n",
    "vector=vectorizer.fit_transform(df_reviews['Desc']).toarray()\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "lda=decomposition.LatentDirichletAllocation(n_components=10, max_iter=10,learning_method='online',learning_offset=50,n_jobs=1,random_state=21)\n",
    "W1=lda.fit_transform(vector)\n",
    "H1=lda.components_\n",
    "\n",
    "num_words=15\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())\n",
    "\n",
    "top_words=lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words=([top_words(t) for t in H1])\n",
    "topics=[' '.join(t) for t in topic_words]\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "print(topics)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "colnames=[\"Topic\"+ str(i) for i in range(lda.n_components)]\n",
    "df_doc_topic=pd.DataFrame(np.round(W1,2), columns=colnames)\n",
    "significant_topic=np.argmax(df_doc_topic.values,axis=1)\n",
    "# df_doc_topic['dominant_topic']=significant_topic\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "df_doc_topic\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "df_doc_topic=df_doc_topic.reset_index(drop=True)\n",
    "df_doc_topic\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# df_topic=df_topic.reset_index(drop=True)\n",
    "# df_topic\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "df_topic_table=pd.concat([df_reviews,df_doc_topic],axis=1)\n",
    "df_topic_table\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "# df_topic_table=df_topic_table.drop('comments',axis=1)\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "df_topic_table=df_topic_table.groupby(['Ticketid']).mean().reset_index()\n",
    "df_topic_table\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "# print(df_reviews)\n",
    "df_topic_table.to_csv('df_topic_modeling.csv')\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('topic_score.csv')\n",
    "# df_reviews.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 C2M8",
   "language": "python",
   "name": "p3-c2m8-g0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
