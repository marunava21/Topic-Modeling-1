{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c464451d",
   "metadata": {},
   "source": [
    "<font size=5 color=\"blue\">Library and Data Import</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d581d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Data_Cleaning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27009/2711671301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mData_Cleaning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatacleaning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Data_Cleaning'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words as w\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Data_Cleaning import datacleaning, datamapping, datapreprocessing\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "# from NLP import NLP as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc9f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /ifxhome/manna/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "initialize = True\n",
    "# lemmatization\n",
    "if initialize:\n",
    "    nltk.download('words')\n",
    "tickets = pd.read_excel('Ticket Details.xlsx', header=1 , sheet_name = \"Sheet 1\", names=['Ticketid', 'Calendar Week', 'Ticket Created Time', 'Priority', 'Site', 'Problem Description', 'General Category', 'Sub Category', 'Resolution', 'Customer Department', 'Predicted General Category', 'Predicted Sub Category'])\n",
    "#tickets.columns = tickets.columns.str.strip()\n",
    "tickets = pd.DataFrame([tickets['Ticketid'], tickets['Problem Description']]).transpose()\n",
    "tickets = tickets.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1beb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##extraction of problem description\n",
    "# regex_remove_punctuations = r'[~`!@#\\$%\\^&\\*\\(\\)_\\+\\-\\=\\{\\}\\|\\[\\]\\\\:;\"\\'<>\\?,\\.\\/]'\n",
    "# for i in range(tickets.shape[0]):\n",
    "#     a=re.sub(r'(\\([2-9]\\))[\\s?[a-zA-Z]\\W*\\D*\\S*]*', '',tickets.loc[i]['Problem Description'])\n",
    "#     b = re.sub(r'(Lot Number:)(\\s*[a-zA-Z]*\\W*\\D*\\S*)*','', a)\n",
    "#     c = re.sub(r'(\\(1\\))(\\s*[a-zA-Z]*\\W*\\D*\\S*)(error\\))','', b)\n",
    "#     text = c.replace('\\n', ' ')\n",
    "#     text = text.replace('\\t', ' ')\n",
    "#     text = text.replace(r'(\\'*\\')', '')\n",
    "#     text = text.strip()\n",
    "# #     text = re.sub(regex_remove_punctuations,' ', text)\n",
    "#     text = text.lower()\n",
    "#     tickets.loc[i]['Problem Description'] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fba296",
   "metadata": {},
   "source": [
    "<font size=5> Initial data cleaning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3da7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileref = 'lookup.xlsx'\n",
    "cleaned = datacleaning(tickets)\n",
    "processed = datamapping(cleaned,fileref)\n",
    "# processed = datapreprocessing(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e32411",
   "metadata": {},
   "outputs": [],
   "source": [
    "## focusing on the problem description\n",
    "processed= pd.DataFrame([processed['Ticketid'], processed['Desc']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4a6206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed.dropna(subset=['Desc'], inplace=True)\n",
    "# processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211522fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text):\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        for word in i.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd35b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processed.Desc\n",
    "\n",
    "counter = counter_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9550a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(counter)\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bebb513d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticketid</th>\n",
       "      <th>Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC000004851418</td>\n",
       "      <td>the fajob does not belong to this equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INC000004851509</td>\n",
       "      <td>error during trackout camstar total ndpw item ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INC000004852100</td>\n",
       "      <td>cannot move out lot strip map total quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INC000004852981</td>\n",
       "      <td>missing next step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INC000004853010</td>\n",
       "      <td>the target step could not be determined for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>INC000006004547</td>\n",
       "      <td>user request to raise camstar it ticket user i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>INC000006004548</td>\n",
       "      <td>buttons are grey out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>INC000006004924</td>\n",
       "      <td>original quantityity of the lot to be tracked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>INC000006005058</td>\n",
       "      <td>lot is not allowed for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8703</th>\n",
       "      <td>INC000006005065</td>\n",
       "      <td>lot complete but active in camstar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8704 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Ticketid                                               Desc\n",
       "0     INC000004851418        the fajob does not belong to this equipment\n",
       "1     INC000004851509  error during trackout camstar total ndpw item ...\n",
       "2     INC000004852100      cannot move out lot strip map total quantity \n",
       "3     INC000004852981                                  missing next step\n",
       "4     INC000004853010  the target step could not be determined for th...\n",
       "...               ...                                                ...\n",
       "8699  INC000006004547  user request to raise camstar it ticket user i...\n",
       "8700  INC000006004548                               buttons are grey out\n",
       "8701  INC000006004924  original quantityity of the lot to be tracked ...\n",
       "8702  INC000006005058                             lot is not allowed for\n",
       "8703  INC000006005065                 lot complete but active in camstar\n",
       "\n",
       "[8704 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.reset_index(inplace=True)\n",
    "processed.drop(\"level_0\", inplace=True, axis=1)\n",
    "processed.drop(\"index\", inplace=True, axis=1)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49d9359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to do it at the beginning\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "def remove_uninfo_word(sen):\n",
    "    tokens = nltk.word_tokenize(sen)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    res = [\"\".join(tups[0]) for tups in tagged if not re.match(r\"^VBP$\", tups[1]) and not re.match(r\"^WDT$\", tups[1]) and not re.match(r\"^VBD$\", tups[1]) \\\n",
    "                                                         and not re.match(r\"^CC$\", tups[1]) and not re.match(r\"^DT$\", tups[1]) \\\n",
    "                                                        and not re.match(r\"^VBZ$\", tups[1]) and not re.match(r\"^com$\", tups[0]) and\\\n",
    "                                                        not re.match(r\"^MD$\", tups[1]) and not re.match(r\"^IN$\", tups[1]) \\\n",
    "                                                           and not re.match(r\"^TO$\", tups[1]) and not re.match(r\"^PRP$\", tups[1]) \\\n",
    "                                                           and not re.match(r\"^user$\", tups[0]) and not re.match(r\"^infineon$\", tups[0])\n",
    "                                                        and not re.match(r\"^lot$\", tups[0]) and not re.match(r\"^does$\", tups[0])\\\n",
    "                          and not re.match(r\"^did$\", tups[0]) and not re.match(r\"^didn't$\", tups[0]) and not re.match(r\"^camstar$\", tups[0])]\n",
    "#     print(tagged)\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab21ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized=processed.copy()\n",
    "lemmatized[\"desc\"]=\"\"\n",
    "lemmatized[\"desc\"] = lemmatized.Desc.map(lambda x: remove_uninfo_word(x))\n",
    "lancaster = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "for i in range(lemmatized.shape[0]):\n",
    "    text_list = lemmatized[\"desc\"].loc[i].split(\" \")\n",
    "#     text_list = [lancaster.stem(word) for word in text_list]\n",
    "    text_list = [lem.lemmatize(word) for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'v') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'a') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'r') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 's') for word in text_list]\n",
    "    lemmatized[\"desc\"].loc[i]= \" \".join(text_list)\n",
    "# lemmatized[\"Desc\"] = lemmatized.Desc.map(lambda x: lancaster.stem(x))\n",
    "# lemmatized[\"Desc\"] = lemmatized.Desc.map(lambda x: lemmatizer.lemmatize(x))\n",
    "# lemmatized = processed.copy()\n",
    "# processed[:100].to_excel(\"new_ticket.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5e62a",
   "metadata": {},
   "source": [
    "<font size=3 color=\"grey\">Just for trial</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb99e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(processed[\"Desc\"].loc[26])\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "res1=\"\"\n",
    "res = [\"\".join(tups[0]) for tups in tagged if not re.match(r\"^VBP$\", tups[1]) and not re.match(r\"^WDT$\", tups[1]) and not re.match(r\"^VBD$\", tups[1]) \\\n",
    "                                                         and not re.match(r\"^CC$\", tups[1]) and not re.match(r\"^DT$\", tups[1]) \\\n",
    "                                                        and not re.match(r\"^VBZ$\", tups[1]) and not re.match(r\"^com$\", tups[0]) and\\\n",
    "                                                        not re.match(r\"^MD$\", tups[1]) and not re.match(r\"^IN$\", tups[1]) \\\n",
    "                                                           and not re.match(r\"^TO$\", tups[1]) and not re.match(r\"^PRP$\", tups[1]) \\\n",
    "                                                           and not re.match(r\"^user$\", tups[0]) and not re.match(r\"^infineon$\", tups[0])\n",
    "                                                        and not re.match(r\"^lot$\", tups[0]) and not re.match(r\"^does$\", tups[0])\\\n",
    "                          and not re.match(r\"^did$\", tups[0]) and not re.match(r\"^didn't$\", tups[0]) and not re.match(r\"^camstar$\", tups[0])]\n",
    "print(tagged)\n",
    "\" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed[processed[\"Ticketid\"]==\"INC000004855678\"][\"Desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192842c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ef935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processed.Desc[0])\n",
    "# print(processed_seq[0])\n",
    "# reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# def decode(text):\n",
    "#     return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "# decode(processed_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets_new = pd.read_excel('new_ticket_me.xlsx')\n",
    "# tickets_new = tickets_new.dropna()\n",
    "# tickets_new['label']=tickets_new['Label'].astype('int')\n",
    "# tickets_new=pd.DataFrame([tickets_new['Desc'], tickets_new['label']]).transpose()\n",
    "# (xtrain,xtest,ytrain,ytest)=train_test_split(tickets_new[\"Desc\"], tickets_new[\"label\"], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e84679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=num_words)\n",
    "# tokenizer.fit_on_texts(xtrain)\n",
    "# word_index = tokenizer.word_index\n",
    "# processed_seq = tokenizer.texts_to_sequences(xtrain)\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# processed_pad = pad_sequences(\n",
    "#     processed_seq, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    "# )\n",
    "# test_sequences = tokenizer.texts_to_sequences(xtest)\n",
    "# test_padded = pad_sequences(\n",
    "#     test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9811601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, BatchNormalization\n",
    "# from keras.initializers import Constant\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(num_words, 32, input_length=max_length))\n",
    "# model.add(LSTM(64, dropout=0.1))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(18, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# optimizer = Adam(learning_rate=4e-4)\n",
    "\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873b87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.asarray(ytrain).astype('int64')\n",
    "# y_test = np.asarray(ytest).astype('int64')\n",
    "\n",
    "##not important one\n",
    "# from sklearn import preprocessing\n",
    "# le= preprocessing.LabelEncoder()\n",
    "# le.fit(y_train)\n",
    "# y_train =  le.fit_transform(y_train)\n",
    "# y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# history = model.fit(\n",
    "#     processed_pad, tf.convert_to_tensor(y_train), batch_size=32, epochs=10, validation_data=(test_padded, tf.convert_to_tensor(y_test)),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_padded[:3], y_test[:3])\n",
    "# test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3c5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29933a0d",
   "metadata": {},
   "source": [
    "<font size=\"5\" color=\"red\">Spell Correction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bacc1",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Created a manual dictionary for the wrong words which i have explored so far. New words can be added later on.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##spell correction step\n",
    "# from nltk.stem import LancasterStemmer\n",
    "# m_dict={\"abrcode\":\"barcode\",\"msg\":\"message\",\"mesg\":\"message\", \"pwd\":\"password\",\"num\":\"number\", \"dn\":\"delivery note\", \"promopt\":\"prompt\",\\\n",
    "#        \"sucess\":\"success\",\"wefer\": \"wafer\",\"aldy\":\"already\",\"quatit\":\"quantity\",\\\n",
    "#        \"qty\":\"quantity\",\"qtys\":\"quantity\",\"quanity\":\"quantity\",\"buttos\":\"buttons\",\"buttin\":\"buttons\",\\\n",
    "#        \"pls\":\"please\",\"pelase\":\"please\",\"txn\":\"transaction\",\"tryin\":\"trying\",\"tring\":\"trying\",\\\n",
    "#        \"remot\":\"remote\",\"exitsts\":\"exists\",\"previos\":\"previous\",\"mvin\":\"movein\",\"alowe\":\"allowed\",\"cariier\":\"carrier\",\\\n",
    "#        \"coloumn\":\"column\",\"produst\":\"product\",\"nto\":\"not\",\"complate\":\"complete\",\"complet\":\"complete\",\"selclected\":\"selected\",\"beccause\":\"because\",\"continu\":\"continue\",\\\n",
    "#        \"atributes\":\"attributes\",\"unabe\":\"unable\",\"unble\":\"unable\",\"erorr\":\"error\",\"movi\":\"move\",\\\n",
    "#        \"gray\":\"grey\",\"greyed\":\"grey\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ca6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import LancasterStemmer\n",
    "# from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster = LancasterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# for i in range(tickets.shape[0]):\n",
    "#     new_l = tickets[\"Problem Description\"].loc[i].split()\n",
    "#     for j, val in enumerate(new_l):\n",
    "#         if val in m_dict:\n",
    "# #             print(val)\n",
    "#             new_l[j]=m_dict[val]\n",
    "#         new_l[j]=lancaster.stem(new_l[j])\n",
    "#         new_l[j]= lemmatizer.lemmatize(new_l[j])\n",
    "#     tickets[\"Problem Description\"].loc[i]=\" \".join(new_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f7775",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> A example of the correction is that the below sentence had mesg in the original problem description and now the \n",
    "message is replaced with the wrong one </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923956a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets[\"Problem Description\"].loc[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5e441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tickets[tickets[\"Ticketid\"]==\"INC000004861950\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets[:101].to_excel(\"tickets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d19767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets_new = pd.read_excel('tickets.xlsx', names=[\"Ticketid\",\"Desc\", \"label\"])\n",
    "# tickets_new = tickets_new.dropna()\n",
    "# tickets_new['label']=tickets_new['label'].astype('int')\n",
    "# tickets_new=pd.DataFrame([tickets_new['Desc'], tickets_new['label']]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4322bf",
   "metadata": {},
   "source": [
    "<font size=5 color=\"green\">Finding important words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8b27d",
   "metadata": {},
   "source": [
    "<font color=\"red\"> I am trying to find some potential problem words for word embedding</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets['json']=\"\"\n",
    "# tickets.dropna(subset=['Desc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946b6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from spacy.tokens import Doc\n",
    "# from spacy.training import Example\n",
    "# import spacy\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# # import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tickets.loc[22]['json']\n",
    "# l = [\"grey\",\"out\", \"button\",\"gray\", \"does\",\"not\",\"belong\", \"mismatch\",\"does\", \"its\", \"lot\",\"qty\",\n",
    "#     \"match\",\"qty\",\"quantity\", \"sequencing\",\"fail\",\"missing\",\"next\",\"step\", \"could\",\"not\", \"be\",\"determined\"]\n",
    "# tickets1=pd.DataFrame(columns=['Ticketid', 'Problem Description', 'json'])\n",
    "# for i in range(tickets.shape[0]):\n",
    "#     tickets['json'].loc[i]=[]\n",
    "#     for k in l:\n",
    "#         print(k)\n",
    "#         if k in tickets['Desc'].loc[i]:\n",
    "#             if len(tickets['json'].loc[i])==0 and len(tickets['Desc'].loc[i])!=0:\n",
    "# #                 tickets['json'].loc[i]['text']=tickets['Problem Description'].loc[i]\n",
    "# #                 start = tickets['Problem Description'].loc[i].find(k)\n",
    "# #                 end = start + len(k)\n",
    "# #                 l1=(start, end, \"PROBLEM\")\n",
    "#                 tickets['json'].loc[i]=[k]\n",
    "#             elif len(tickets['json'].loc[i])!=0 and len(tickets['Desc'].loc[i])!=0:\n",
    "# #                 start = tickets['Problem Description'].loc[i].find(k)\n",
    "# #                 end = start + len(k)\n",
    "# #                 l1=(start, end, \"PROBLEM\")\n",
    "#                 tickets['json'].loc[i].append(k)\n",
    "#     if len(tickets['json'].loc[i])==0:\n",
    "#         tickets['json'].loc[i]=\"\"\n",
    "# #     if len(tickets['json'].loc[i])!=0:\n",
    "# #         tickets['json'].loc[i]=json.dumps(list(tickets['json'].loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487c9b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tickets.reset_index(inplace=True)\n",
    "# tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets1=tickets.copy()\n",
    "# tickets1.dropna(subset=['json'], inplace=True)\n",
    "# tickets.to_excel('tickets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590307c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db2b850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "lemmatized.reset_index(inplace=True)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatized[\"common_words\"]=\"\"\n",
    "for i in range(lemmatized.shape[0]):\n",
    "    doc = nlp(lemmatized['desc'].loc[i])\n",
    "    # all tokens that arent stop words or punctuations\n",
    "    words = [token.text\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct]\n",
    "    # five most common tokens\n",
    "    word_freq = Counter(words)\n",
    "    common_words = word_freq.most_common()\n",
    "\n",
    "    lemmatized[\"common_words\"].loc[i]=[i[0] for i in common_words]\n",
    "    lemmatized[\"common_words\"].loc[i]=\" \".join(lemmatized[\"common_words\"].loc[i])\n",
    "lemmatized.to_excel(\"tickets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lNonDicW=[]\n",
    "# # english_words = set(nltk.corpus.words.words())\n",
    "# from nltk.corpus import wordnet\n",
    "# for i in range(tickets.shape[0]):\n",
    "#     doc = nlp(tickets['Problem Description'].loc[i])\n",
    "#     for token in doc:\n",
    "#         if not wordnet.synsets(token.text) and not token.is_stop and not token.is_punct:\n",
    "#             lNonDicW.append((token.text,tickets['Problem Description'].loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2=[]\n",
    "# l_badwords=['qty', 'camstar', 'infineon', 'com','boc','admin']\n",
    "# for i in lNonDicW:\n",
    "#     if re.match(r'\\s+', i[0]) or re.match(r'\\w*\\d+',i[0]) or i[0] in l_badwords:\n",
    "#         pass\n",
    "#     elif re.match(r'[a-z]+', i[0]):\n",
    "#         l2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd00490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(l2).to_excel(\"nondictwords.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cbb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472123d0",
   "metadata": {},
   "source": [
    "<font size=5 color=\"red\">WordNet part</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e875585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a467fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyStanfordDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb1726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74b1c1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index           0\n",
       "Ticketid        0\n",
       "Desc            0\n",
       "desc            0\n",
       "common_words    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d12bd583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                            8704\n",
       "unique                                           5995\n",
       "top       the fajob does not belong to this equipment\n",
       "freq                                              401\n",
       "Name: Desc, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed['Desc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8bbec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram=1):\n",
    "    words=[word for word in text.split(\" \") if word not in set(stopwords.words('english'))]  \n",
    "    print(\"Sentence after removing stopwords:\",words)\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba3d91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removing stopwords: ['trackin', 'trackout', 'move', 'button', 'grey', 'fajob', 'transaction']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trackin trackout move',\n",
       " 'trackout move button',\n",
       " 'move button grey',\n",
       " 'button grey fajob',\n",
       " 'grey fajob transaction']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_N_grams(processed.loc[126]['Desc'],3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 C2M8 Oracle",
   "language": "python",
   "name": "p3-c2m8-g0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
