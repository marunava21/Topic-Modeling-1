{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2801986",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /ifxhome/manna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /ifxhome/manna/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /ifxhome/manna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/fibebocai/virtualenvs/fibebocai_text/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fajob', 'belong', 'equipment']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31258/4277790558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "\"\"\"Airbnb_TopicModel_Visualisation.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/10N8imK1z9b8CkAbl1WAHqYbW1kjZaf16\n",
    "\"\"\"\n",
    "from Data_Cleaning import datacleaning, datamapping, datapreprocessing\n",
    "# !pip install pyldavis\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive')\n",
    "df_reviews=pd.read_excel('tickets (6).xlsx')\n",
    "df_reviews\n",
    "\n",
    "df_reviews.dropna(subset=['Desc'], how='all', inplace=True)\n",
    "\n",
    "#remove all languages except english\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def keep_english(x):\n",
    "    englishwords=\" \".join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in words or not w.isalpha())\n",
    "    return englishwords\n",
    "\n",
    "# df_reviews['Desc']=df_reviews['Desc'].apply(keep_english)\n",
    "df_reviews['Desc'] = df_reviews['Desc'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "df_reviews['Desc']\n",
    "\n",
    "import glob\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "# !pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words(\"english\")\n",
    "stopwords.extend(['from', 'subject', 're', 'edu', 'use', 'infineon', 'camstar','com', 'screenshot','yoda','lot','iptester',\\\n",
    "                 \"mkz\",\"mkzsapa\",\"mkzsapaxx\",\"site\",\"xml\",\"number\",\"affect\",\"user\",\"error\",\"etc\"])\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "# lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final=[]\n",
    "    for text in texts:\n",
    "        new=gensim.utils.simple_preprocess(text,deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "fileref = 'lookup.xlsx'\n",
    "# cleaned = datacleaning(tickets)\n",
    "processed = datamapping(df_reviews,fileref)\n",
    "data_words=gen_words(processed['Desc'])\n",
    "data_words = remove_stopwords(data_words)\n",
    "lem = WordNetLemmatizer()\n",
    "for i in range(len(data_words)):\n",
    "    text_list = data_words[i]\n",
    "#     text_list = [lancaster.stem(word) for word in text_list]\n",
    "    text_list = [lem.lemmatize(word) for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'v') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'a') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 'r') for word in text_list]\n",
    "    text_list = [lem.lemmatize(word, 's') for word in text_list]\n",
    "    df_reviews[\"desc\"].loc[i]= \" \".join(text_list)\n",
    "\n",
    "\n",
    "# lemmatized_texts=df_reviews['desc']\n",
    "\n",
    "#get word tokens\n",
    "\n",
    "lookup = fileref\n",
    "typo = pd.read_excel(lookup, sheet_name=\"typo\")\n",
    "worddict = dict(zip(typo.words, typo.correction))\n",
    "df_reviews.desc = df_reviews.desc.replace(worddict, regex=True)\n",
    "data_words=gen_words(df_reviews['desc'])\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "#BIGRAMS AND TRIGRAMS\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_words)\n",
    "\n",
    "print (data_bigrams_trigrams[0][0:20])\n",
    "\n",
    "id2word=corpora.Dictionary(data_words)\n",
    "corpus=[]\n",
    "\n",
    "for text in data_words:\n",
    "    new=id2word.doc2bow(tokenize(text))\n",
    "    corpus.append(new)\n",
    "    \n",
    "print(corpus[0][0:20])\n",
    "\n",
    "# lda_model=gensim.models.ldamodel.LdaModel(corpus=corpus[:-1],\n",
    "#                                          id2word=id2word,\n",
    "#                                          num_topics=12,\n",
    "#                                          random_state=101,\n",
    "#                                          update_every=1,\n",
    "#                                          chunksize=100,\n",
    "#                                          passes=10,\n",
    "#                                          alpha='auto')\n",
    "\n",
    "\n",
    "# lda_model = gensim.models.lsimodel.LsiModel(\n",
    "# corpus=corpus, id2word=id2word, num_topics=10,chunksize=100\n",
    "# )\n",
    "lda_model = gensim.models.hdpmodel.HdpModel(corpus=corpus, id2word=id2word)\n",
    "\n",
    "test_doc=corpus[-1]\n",
    "\n",
    "vector=lda_model[test_doc]\n",
    "print(vector)\n",
    "\n",
    "\n",
    "#sort words\n",
    "def Sort(sub_li):\n",
    "    sub_li.sort(key=lambda x: x[1])\n",
    "    sub_li.reverse()\n",
    "    return (sub_li)\n",
    "\n",
    "new_vector= Sort(vector)\n",
    "\n",
    "print(new_vector)\n",
    "\n",
    "#Visualize\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis=pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word,mds='mmds',R=15)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4a2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3431f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*equipment + 0.016*printer + 0.015*server + 0.015*contact + 0.015*area + 0.015*application + 0.015*integration + 0.012*track + 0.009*fajob + 0.009*button'),\n",
       " (1,\n",
       "  '0.006*cannot + 0.006*process + 0.005*equipment + 0.004*track + 0.004*move + 0.004*need + 0.003*state + 0.003*issue + 0.003*mention + 0.003*spec'),\n",
       " (2,\n",
       "  '0.009*test + 0.006*equipment + 0.005*summary + 0.004*miss + 0.004*id + 0.003*till + 0.002*type + 0.002*viewer + 0.002*mode + 0.002*tester'),\n",
       " (3,\n",
       "  '0.004*equipment + 0.004*cannot + 0.003*fajob + 0.002*concern + 0.002*job + 0.002*printer + 0.002*application + 0.002*create + 0.002*fa + 0.002*server'),\n",
       " (4,\n",
       "  '0.004*equipment + 0.004*application + 0.003*test + 0.003*criticl + 0.002*code + 0.002*printer + 0.002*still + 0.002*source + 0.002*client + 0.002*ifxdoc'),\n",
       " (5,\n",
       "  '0.004*equipment + 0.003*object + 0.002*server + 0.002*area + 0.002*application + 0.002*contact + 0.002*printer + 0.002*floor + 0.002*integration + 0.002*ramli'),\n",
       " (6,\n",
       "  '0.003*length + 0.003*equipment + 0.002*ownername + 0.002*paramname + 0.002*application + 0.002*track + 0.002*automatically + 0.002*ccar + 0.002*logic + 0.002*transac'),\n",
       " (7,\n",
       "  '0.003*batch + 0.003*dbdataadapter + 0.002*adapter + 0.002*equipment + 0.002*access + 0.002*inbutton + 0.002*cause + 0.002*datarow + 0.002*extn + 0.002*pref'),\n",
       " (8,\n",
       "  '0.002*easyline + 0.002*mandatory + 0.002*commandcount + 0.002*equipment + 0.002*visibility + 0.002*tan + 0.002*button + 0.002*quite + 0.002*cap + 0.002*c_testcb'),\n",
       " (9,\n",
       "  '0.006*test + 0.005*equipment + 0.003*summary + 0.002*server + 0.002*microflex + 0.002*etc + 0.002*control + 0.002*area + 0.002*adhimoolam + 0.002*id'),\n",
       " (10,\n",
       "  '0.003*integration + 0.003*equipment + 0.003*consumer + 0.002*guide + 0.002*tell + 0.002*area + 0.002*ifap + 0.002*likely + 0.002*printer + 0.002*team'),\n",
       " (11,\n",
       "  '0.003*equipment + 0.003*printer + 0.003*contact + 0.002*integration + 0.002*batam + 0.002*continuously + 0.002*stril + 0.002*data + 0.002*area + 0.002*add'),\n",
       " (12,\n",
       "  '0.005*hold + 0.003*ddm + 0.003*geryed + 0.002*release + 0.002*equipment + 0.002*allow + 0.002*attachedl + 0.002*transact + 0.002*integration + 0.002*child'),\n",
       " (13,\n",
       "  '0.002*prior + 0.002*process + 0.002*cumulative + 0.002*mtu + 0.002*personal + 0.002*container + 0.002*equipment + 0.002*compound + 0.002*employee + 0.002*waste'),\n",
       " (14,\n",
       "  '0.003*equipment + 0.002*detect + 0.002*seem + 0.002*printer + 0.002*buy + 0.002*track + 0.002*trackoutxml + 0.002*locatios + 0.002*deliverred + 0.002*inn'),\n",
       " (15,\n",
       "  '0.003*hold + 0.003*quantityity + 0.002*autovision + 0.002*equipment + 0.002*printer + 0.002*spcviolation + 0.002*total + 0.002*impl + 0.002*cannot + 0.002*cancelados'),\n",
       " (16,\n",
       "  '0.003*printer + 0.003*equipment + 0.002*clerance + 0.002*leader + 0.002*server + 0.002*automatic + 0.002*integration + 0.002*cerrados + 0.002*path + 0.002*spacer_gt'),\n",
       " (17,\n",
       "  '0.003*visibl + 0.003*equipment + 0.002*reset + 0.002*grey + 0.002*directly + 0.002*assy_to_test + 0.002*wrong + 0.002*forward + 0.002*full + 0.002*reseved'),\n",
       " (18,\n",
       "  '0.003*equipment + 0.003*move + 0.002*notprocesssed + 0.002*havent + 0.002*greyout + 0.002*summary + 0.002*downcard + 0.002*np + 0.002*undefined + 0.002*insertion'),\n",
       " (19,\n",
       "  '0.004*track + 0.003*equipment + 0.002*grey + 0.002*button + 0.002*step + 0.002*move + 0.002*pob + 0.002*noorfazilah + 0.002*employeename + 0.002*semifinish')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d629a41f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh.util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17588/1451058490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get topic weights and dominant topics ------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/bokeh/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# expose Bokeh version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# expose sample data module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh.util'"
     ]
    }
   ],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 C2M8",
   "language": "python",
   "name": "p3-c2m8-g0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
